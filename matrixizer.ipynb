{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib2\n",
    "import numpy as np\n",
    "import cStringIO as StringIO\n",
    "import codecs\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "source_url = r'https://data.seattle.gov/api/views/mags-97de/rows.json?accessType=DOWNLOAD'\n",
    "\n",
    "response = urllib2.urlopen(source_url)\n",
    "source = response.read()\n",
    "full_data = json.loads(source)\n",
    "\n",
    "# split into metadata (column name descriptions, etc) and actual data\n",
    "meta = full_data['meta']\n",
    "data = full_data['data']\n",
    "\n",
    "# column names available\n",
    "names = [m['name'].lower() for m in meta['view']['columns']]\n",
    "\n",
    "name_to_idx_dict = {}\n",
    "for idx, name in enumerate(names):\n",
    "    name_to_idx_dict[name] = idx\n",
    "\n",
    "# indices of column names we are interested in    \n",
    "description_idx = name_to_idx_dict['description']\n",
    "category_idx    = name_to_idx_dict['category']\n",
    "id_idx          = name_to_idx_dict['id']\n",
    "lon_idx         = name_to_idx_dict['longitude']\n",
    "lat_idx         = name_to_idx_dict['latitude']\n",
    "value_idx       = name_to_idx_dict['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map building permit categories to numeric id's\n",
    "def category_to_id(category):\n",
    "    if category   == 'SINGLE FAMILY / DUPLEX':\n",
    "        return 1\n",
    "    elif category == 'MULTIFAMILY':\n",
    "        return 2\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# collect permit id's, their description texts, categories, fabricated category ids, lons, lats, values into lists \n",
    "id_list          = []\n",
    "description_list = []\n",
    "category_list    = []\n",
    "category_id_list = []\n",
    "lon_list         = []\n",
    "lat_list         = []\n",
    "value_list       = []\n",
    "\n",
    "\n",
    "for d in data:\n",
    "    if d is not None:\n",
    "        id_list.append(d[id_idx])\n",
    "        description_list.append(d[description_idx])\n",
    "        category_list.append(d[category_idx])\n",
    "        category_id_list.append(category_to_id(d[category_idx]))\n",
    "        lon_list.append(d[lon_idx])\n",
    "        lat_list.append(d[lat_idx])\n",
    "        value_list.append(d[value_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# regular-expression-based text cleaners\n",
    "# XXX add more\n",
    "\n",
    "# get rid of numerals in the terms\n",
    "number_pat = re.compile('\\D*\\d')\n",
    "def not_match_number(x):\n",
    "    if re.match(number_pat, x) is None:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the list of clean description texts\n",
    "text_list = []\n",
    "\n",
    "indices = []\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "# XXX investigate exeptions\n",
    "for i, d in enumerate(description_list):\n",
    "    try:\n",
    "        words = d.lower().split(' ')\n",
    "        update = filter(not_match_number, words)\n",
    "        text = ' '.join([stemmer.stem(word) for word in update])\n",
    "        text_list.append(text)\n",
    "        indices.append(i)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "identity_list    = list(np.array(id_list)[indices])\n",
    "category_id_list = list(np.array(category_id_list, dtype=np.int32)[indices])\n",
    "lon_list         = list(np.array(lon_list)[indices])\n",
    "lat_list         = list(np.array(lat_list)[indices])\n",
    "value_list       = list(np.array(value_list)[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct the TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "X = vectorizer.fit_transform(text_list)\n",
    "\n",
    "# vocabulary\n",
    "term_list = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save TF-IDF matrix\n",
    "\n",
    "buf = StringIO.StringIO()\n",
    "\n",
    "for row, x in enumerate(X):\n",
    "    dense_x = x.toarray().flatten()\n",
    "    idx = np.where(dense_x != 0)[0]\n",
    "    val = dense_x[idx]\n",
    "    print >> buf, '%d' % category_id_list[row],\n",
    "    for i, v in zip(*[idx, val]):\n",
    "        print >> buf, ' %d:%f' % (i, v),\n",
    "    print >> buf\n",
    "\n",
    "contents = buf.getvalue()\n",
    "f = open('seattle_tfidf.txt', 'w')\n",
    "f.write(contents)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save vocabulary terms \n",
    "\n",
    "term_list = vectorizer.get_feature_names()\n",
    "f = codecs.open('seattle_terms.txt', 'w', 'utf-8')\n",
    "for term in term_list:\n",
    "    print >> f, '%s' % term\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save building permit id's\n",
    "\n",
    "f = codecs.open('seattle_identities.txt', 'w', 'utf-8')\n",
    "for identity in identity_list:\n",
    "    print >> f, '%s' % identity\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save building locations\n",
    "\n",
    "f = codecs.open('seattle_locations.txt', 'w', 'utf-8')\n",
    "for lon, lat in zip(*[lon_list, lat_list]):\n",
    "    print >> f, '%s %s' % (lon, lat)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save values\n",
    "\n",
    "f = codecs.open('seattle_values.txt', 'w', 'utf-8')\n",
    "for val in value_list:\n",
    "    print >> f, '%s' % val\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the matrix as in sparse CSR format\n",
    "def save_sparse_csr(filename,array):\n",
    "    np.savez(filename, data = array.data ,indices=array.indices,\n",
    "             indptr =array.indptr, shape=array.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_sparse_csr('seattle_tfidf_csr_matrix.npz', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
